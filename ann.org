#+TITLE: Artificial Neural Network From Scratch

* A Simple And Intuitive Guide To Neural Networks (Python, HTML, Javascript)

You can create a webpage in HTML and Javascript that will let you draw a number, and you can write the server software and a neural network to tell you what number you drew.

I won't use words like "differentiable".

* Iteration 1

** HTML

We need something to draw on.

If you're not familiar with what a "pixel" is, take a moment to look that up.

We are going to start with the simplest "canvas" that can represent "one" and "zero" in any meaningful way.

It will be canvas of 3 pixels wide by 3 pixels high.

#+begin_example
000
000
000
#+end_example

Our "pencil" will turn the pixels from 0 to 1.

An ideal zero in this 3x3 grid will look like this:

#+begin_example
111
101
111
#+end_example

And an ideal one will look like this:

#+begin_example
010
010
010
#+end_example

But since we are presenting this canvas to users to draw on, the pixels can't literally be single pixels.

We can take a chunk of pixels on screen, a 50x50 square of pixels for example, and treat that entire chunk as a single pixel.

So we'll display a 3x3 grid that represents 9 of our pixels, but they will be represented on screen as a 3x3 grid of 50x50 "actual monitor" pixels.

In the end, it will look like this:

TODO: INSERT IFRAME

The details of the pixel coloring/scaling will be handled in some Javascript later. For now, let's just create an HTML file to work with.

We'll need:

- Canvas on which to draw numbers
- Buttons to:
  - reset the canvas
  - add the drawing and its labeled digit to the training data
  - predict what number the drawing represents

#+BEGIN_SRC html :tangle ocr.html :mkdirp yes :noweb yes
<html>
  <head>
    <meta charset="UTF-8">
    <script src="ocr.js" type="text/javascript"></script>
    <link rel="stylesheet" href="ocr.css" type="text/css" media="screen" />
    <title>OCR Demo</title>
  </head>
  <body onload="ocrDemo.onLoadFunction()">
    <div id="main-container" style="text-align: center;">
      <h1>OCR Demo</h1>
      <canvas id="canvas" width="200" height="200"></canvas>
      <form name="input">
        <label for="digit">Digit:</label>
        <input type="text" id="digit">
        <input type="button" value="Train" onclick="ocrDemo.train()">
        <input type="button" value="Predict" onclick="ocrDemo.predict()">
        <input type="button" value="Reset" onclick="ocrDemo.resetCanvas()">
      </form>
    </div>
  </body>
</html>
#+END_SRC

* Javascript

You'll see the stylesheet linked in the HTML is called ~ocr.js~. Let's create that.

In the body tag, we have an ~onload~ handler that calls ~ocrDemo.onLoadFunction()~. That will
be the start of the ~ocr.js~ file. But we'll finish the details later.

Javascript treats everything defined outside of a function as being in the global namespace. It's a bad habit to pollute the global namespace with things that aren't necessary. So we'll put most of our code inside a function and only put what we absolutely need to in the global namespace.

#+NAME: ocr.js
#+BEGIN_SRC javascript :tangle ocr.js :noweb no-export
let ocrDemo = (function() {
    <<javascript constants>>
    <<drawGrid>>
    <<fillSquare>>
    <<mouse events>>
    <<onLoadFunction>>
    <<resetCanvas>>
    <<sendData>>
    <<train>>
    <<predict>>
    return {
        onLoadFunction,
        train,
        predict,
        resetCanvas
    };
})();
#+END_SRC

** Drawing the grid on canvas

Let's define some constants for coloring the canvas.

#+BEGIN_SRC javascript :noweb-ref "javascript constants"
const GRID_STROKE_COLOR = "blue";
const BACKGROUND_COLOR = "black";
const STROKE_COLOR = "white";
const PIXEL_WIDTH = 50;
const GRID_WIDTH = 3;
const CANVAS_WIDTH = PIXEL_WIDTH * GRID_WIDTH;
const HOST = "localhost";
const PORT = "8888";
#+END_SRC

Any time we draw a fresh grid, we'll want to fill the entire canvas with the background color and then re-draw the lines of the grid.

#+NAME: drawGrid
#+BEGIN_SRC javascript
function drawGrid(ctx) {
    ctx.fillStyle = BACKGROUND_COLOR;
    ctx.fillRect(0, 0, CANVAS_WIDTH, CANVAS_WIDTH);
    ctx.strokeStyle = GRID_STROKE_COLOR;
    for (
        let x = PIXEL_WIDTH, y = PIXEL_WIDTH;
        x < CANVAS_WIDTH;
        x += PIXEL_WIDTH, y += PIXEL_WIDTH
    ) {
        ctx.beginPath();
        ctx.moveTo(x, 0);
        ctx.lineTo(x, CANVAS_WIDTH);
        ctx.moveTo(0, y);
        ctx.lineTo(CANVAS_WIDTH, y);
        ctx.stroke();
    }
};
#+END_SRC

#+NAME: draw grid example
#+BEGIN_SRC html :tangle drawGridExample.html :noweb no-export
<html>
  <body>
    <canvas id="canvas" width="200" height="200"></canvas>
    <script>
      <<javascript constants>>
      <<drawGrid>>
      let canvasEl = document.getElementById("canvas");
      let context = canvasEl.getContext("2d");
      drawGrid(context);
    </script>
  </body>
</html>
#+END_SRC

Let's see what the grid looks like at [[./drawGridExample.html]]

#+begin_export html
<iframe src="drawGridExample.html"></iframe>
#+end_export

** Drawing on the grid

Let's write a function that will take a mouse event and a canvas element and will fill a square of the grid.

Mouse events have a ~clientX~ and ~clientY~ property that represent the pixel coordinates of the mouse. Those coordinates are relative to the applications viewport. For example, clicking on the left edge of the viewable area of a page will always result in a mouse event with a ~clientX~ of ~0~, regardless of whether the page is scrolled horizontally.

We also need to know where the canvas element is in relation to the viewport so that we can draw in the correct square of the grid. Elements have a ~getBoundingClientRect()~ function that returns an object that has ~x~ and ~y~ properties denoting the pixel positions of their left-most and top-most edges.

If the left edge of the canvas is ~100~ pixels to the right of the left edge of the viewport, and if a mouse event has a ~clientX~ between ~100~ and ~120~, then we know the mouse is in the first column of squares of the grid.

#+NAME: fillSquare
#+BEGIN_SRC javascript :noweb yes
function fillSquare(mouseEvent, context, canvasElement) {
    let boundingRect = canvasElement.getBoundingClientRect();
    let x = mouseEvent.clientX - boundingRect.x;
    let y = mouseEvent.clientY - boundingRect.y;
    let xPixel = Math.floor(x / PIXEL_WIDTH);
    let yPixel = Math.floor(y / PIXEL_WIDTH);
    context.fillStyle = STROKE_COLOR;
    context.fillRect(
        xPixel * PIXEL_WIDTH,
        yPixel * PIXEL_WIDTH,
        PIXEL_WIDTH,
        PIXEL_WIDTH
    );
    <<save info about which pixels are colored in>>
}
#+END_SRC

#+NAME: mouse events
#+BEGIN_SRC javascript
function onMouseDown(event, context, canvas) {
    canvas.isDrawing = true;
    fillSquare(event, context, canvas);
}

function onMouseUp(canvas) {
    canvas.isDrawing = false;
}

function onMouseMove(event, context, canvas) {
    if (!canvas.isDrawing) {
        return;
    }
    fillSquare(event, context, canvas);
}
#+END_SRC

Let's try it out! See [[./mouseEventsExample.html]]

#+BEGIN_SRC html :tangle mouseEventsExample.html :noweb no-export
<html>
  <body>
    <canvas id="canvas" width="200" height="200"></canvas>
    <script>
      <<javascript constants>>
      <<drawGrid>>
      let canvasEl = document.getElementById("canvas");
      let context = canvasEl.getContext("2d");
      drawGrid(context);

      <<fillSquare>>
      <<mouse events>>
      canvasEl.onmousemove = function(event) { onMouseMove(event, context, canvasEl); };
      canvasEl.onmousedown = function(event) { onMouseDown(event, context, canvasEl); };
      canvasEl.onmouseup = function(_) { onMouseUp(canvasEl); };
    </script>
  </body>
</html>
#+END_SRC

We also need a variable to store the information regarding which pixels make up the drawn number.

We can imagine each row in the grid as being a list of values of either ~1~ or ~0~.

If there is ink in the pixel, then the pixel will be represented in the list as ~1~.

If there is no ink coloring the pixel, then the value will be ~0~.

So, if our zero looks like this:

#+begin_example
111
101
111
#+end_example

Then the list representing the first row will be:

#+begin_example
[1, 1, 1]
#+end_example

And the list representing the second row will be:

#+begin_example
[1, 0, 1]
#+end_example

And we can combine each of the three rows into an list of lists:

#+begin_example
[[1, 1, 1],
 [1, 0, 1],
 [1, 1, 1]]
#+end_example

We may eventually need to treat this data in a different structure. But this list of lists is convenient for now.

It will start off empty. Every pixel will have a value of ~0~, representing that there is no "ink" on the canvas. No number has been drawn.

#+BEGIN_SRC javascript :noweb-ref "javascript constants"
let pixelData = [[0, 0, 0],
                 [0, 0, 0],
                 [0, 0, 0]];
#+END_SRC

Along with coloring the square in the grid, we also want to store the information that we colored a particular pixel in our pixelData that we'll later send to a server to either train our model or make a prediction.

#+NAME: save info about which pixels are colored in
#+begin_src javascript
let pixelIndex = yPixel * GRID_WIDTH + xPixel;
pixelData[pixelIndex] = 1;
#+end_src

Now we have everything we need to complete our ~onLoadFunction~. In it, we'll do all our initial one-time setup: draw the grid and establish mouse events.

#+NAME: onLoadFunction
#+BEGIN_SRC javascript :noweb no-export
function onLoadFunction() {
    resetCanvas();
    let canvasEl = document.getElementById("canvas");
    let context = canvasEl.getContext("2d");
    canvasEl.onmousemove = function(event) { onMouseMove(event, context, canvasEl); };
    canvasEl.onmousedown = function(event) { onMouseDown(event, context, canvasEl); };
    canvasEl.onmouseup = function(_) { onMouseUp(canvasEl); };
}
#+END_SRC

For the functionality of clearing the canvas to reset our drawing, we'll simple re-draw the grid and clear out the variable that stores which squares of the grid were colored.

#+NAME: resetCanvas
#+BEGIN_SRC javascript
function resetCanvas() {
    let canvasEl = document.getElementById("canvas");
    let context = canvasEl.getContext("2d");
    let gridSize = Math.pow((CANVAS_WIDTH / PIXEL_WIDTH), 2);
    pixelData = [];
    while (gridSize--) pixelData.push(0);
    console.log(pixelData);
    drawGrid(context);
}
#+END_SRC

** Sending data to the server

#+NAME: server communication
#+BEGIN_SRC javascript
<<sendData>>
<<train>>
<<predict>>
#+END_SRC

#+NAME: sendData
#+BEGIN_SRC javascript
function sendData(path, json) {
    let xhr = new XMLHttpRequest();
    xhr.open("POST", `http://${HOST}:${PORT}/${path}`);
    xhr.onload = function() {
        if (xhr.status == 200) {
            let responseJSON = JSON.parse(xhr.responseText);
            if (responseJSON && responseJSON.type == "predict") {
                alert(`The neural network predicts you wrote a '${responseJSON.result}'`)
            }
        } else {
            alert(`Server returned status ${xhr.status}.`);
        }
    };
    xhr.onerror = function() {
        alert(`Error occured while connecting to server: ${xhr.target.statusText}`);
    };
    let msg = JSON.stringify(json);
    xhr.setRequestHeader("Content-Length", msg.length);
    xhr.setRequestHeader("Connection", "close");
    xhr.send(msg);
}
#+END_SRC

#+NAME: train
#+BEGIN_SRC javascript
function train() {
    let digitValue = document.getElementById("digit").value;
    if (!digitValue.match(/^\d/)) {
        alert("Please type and draw a digit in order to train the network.");
        return;
    }
    let json = {
        image: pixelData,
        label: digitValue
    };
    sendData("train", json);
}
#+END_SRC

#+NAME: predict
#+BEGIN_SRC javascript
function predict() {
    if (pixelData.indexOf(1) < 0) {
        alert("Please draw a digit in order to use prediction.");
    } else {
        let json = {
            image: pixelData,
        };
        sendData("predict", json);
    }
}
#+END_SRC

* Python Server

#+BEGIN_SRC python :tangle server.py
import http.server
import json
import numpy as np
from functools import partial

import nn

HOST_NAME = "localhost"
PORT_NUMBER = 8888
INPUT_NODE_COUNT = 9
HIDDEN_NODE_COUNT = 6
OUTPUT_NODE_COUNT = 2

neural_network = nn.OCRNeuralNetwork(INPUT_NODE_COUNT, HIDDEN_NODE_COUNT, OUTPUT_NODE_COUNT)


class JSONHandler(http.server.SimpleHTTPRequestHandler):
    def do_POST(self):
        response_code = 200
        response = ""
        content_len = int(self.headers.get("Content-Length", 0))
        content = self.rfile.read(content_len)
        payload = json.loads(content)
        if self.path == "/train":
            neural_network.back_propagate(np.array(payload["image"]), int(payload["label"]))
            response_code = 200
        elif self.path == "/predict":
            response_code = 200
            predictions = neural_network.predict(np.array(payload["image"]))
            print(predictions)
            prediction = max(predictions)
            response = {"type": "predict", "result": predictions.tolist().index(prediction)}
        elif self.path == "/initialize":
            response_code = 200
            neural_network.initialize()
        else:
            response_code = 404
        self.send_response(response_code)
        self.send_header("Content-Type", "application/json")
        self.end_headers()
        if response:
            self.wfile.write(json.dumps(response).encode("utf-8"))

def main():
    print(f"Serving HTTP on {HOST_NAME} port {PORT_NUMBER}")
    httpd = http.server.HTTPServer((HOST_NAME, PORT_NUMBER), partial(JSONHandler, directory="."))
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        pass
    else:
        print("Unexpected server exception occurred.")
    finally:
        httpd.server_close()

if __name__ == "__main__":
    main()
#+END_SRC

* Neural Network

** Requirements

Matplotlib and Numpy are two external requirements we'll need.

#+BEGIN_SRC plaintext :tangle requirements.txt
matplotlib
numpy
#+END_SRC

** What is an Artificial Neural Network?

*** The "Model" *is* the weights...

** Implementing a single Perceptron

** How to tell if data is linealy seperable?

*** XOR example


** Backpropagation
:PROPERTIES:
:header-args:python: :session *backpropagation*
:END:

I want this code to be perfectly repeatable. But since we'll be initializing some random numbers, you're results might vary if you try to run this code. Therefore, let's give Numpy a specific seed for its random number generation so that we all get the same "random" numbers.

#+begin_src python :results none :noweb yes
<<imports>>
<<helpers>>
random_state = RandomState(MT19937(SeedSequence(42)))
#+end_src

We want to predict whether a number is ~0~ or ~1~.

Input is a 3x3 grid. 9 values.

#+begin_src python :results output :noweb yes
ideal_zero = flatten([[1, 1, 1],
                      [1, 0, 1],
                      [1, 1, 1]])
ideal_one  = flatten([[0, 1, 0],
                      [0, 1, 0],
                      [0, 1, 0]])

# Truncated to 2 decimal places for nicer printing for examples.
vector_round = np.vectorize(round)
random_state = RandomState(MT19937(SeedSequence(42)))
weights = vector_round(random_state.rand(2, 9), 2)

# Given the above, we want to adjust our weights such that:
#
# np.dot(weights, ideal_zero) == [1, 0]
# and
# np.dot(weights, ideal_one) == [0, 1]
#
# Or, more accurately...
# prediction = np.dot(weights, ideal_zero)
# prediction.index(max(prediction)) == 0
# prediction = np.dot(weights, ideal_zero)
# prediction.index(max(prediction)) == 1
print(f"Ideal zero: {ideal_zero}")
print(f"Ideal one:  {ideal_one}")
print(f"Weights (random):\n{weights}")
print(f"Zero prediction: {np.dot(weights, ideal_zero)}")
print(f"One prediction:  {np.dot(weights, ideal_one)}")
#+end_src

#+RESULTS:
: Ideal zero: [1 1 1 1 0 1 1 1 1]
: Ideal one:  [0 1 0 0 1 0 0 1 0]
: Weights (random):
: [[0.54 0.62 0.06 0.81 0.86 0.63 0.68 0.68 0.48]
:  [0.73 0.16 0.73 0.22 0.7  0.96 0.28 0.71 0.89]]
: Zero prediction: [4.5  4.68]
: One prediction:  [2.16 1.57]

How can we adjust our weights so that ~np.dot(weigts, ideal_zero)~ is closer to ~[1, 0]~ than ~[0, 1]~?

We could hardcode some weights and get pretty close.

#+begin_src python :results output
hardcoded_weights = np.array([[0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.2, 0.0, 0.2],
                              [0.0, 0.1, 0.0, 0.0, 0.9, 0.0, 0.0, 0.1, 0.0]])
print(f"Zero prediction: {np.dot(hardcoded_weights, ideal_zero)}")
print(f"One prediction:  {np.dot(hardcoded_weights, ideal_one)}")
#+end_src

#+RESULTS:
: Zero prediction: [1.2 0.2]
: One prediction:  [0.  1.1]

This gets us pretty close and is very intuitive.

If we provide weight such that the output neuron for ~1~ gets a value clse to one when the middle pixel is "on", then that gets us close to one.

How do we programatically find the optimal values for those weights?

Well, first we need to know how far away each value in that vector is from our ideal value so that we know which way we need to go.

The function that tells us that is known as the "cost" function.

#+begin_src python :results output
def errors(calculated_values, target_values):
    return target_values - calculated_values

random_state = RandomState(MT19937(SeedSequence(42)))
weights = vector_round(random_state.rand(2, 9), 2)

result_of_zero = np.dot(weights, ideal_zero)
expected_zero_output = np.array([1, 0])
errors_of_zero = errors(result_of_zero, expected_zero_output)
print(f"Result of zero: {result_of_zero}")
print(f"Ideal zero:     {expected_zero_output}")
print(f"Cost of zeros:  {errors_of_zero}")

result_of_one = np.dot(weights, ideal_one)
expected_one_output = np.array([0, 1])
errors_of_one = errors(result_of_one, expected_one_output)
print(f"Result of one: {result_of_one}")
print(f"Ideal one:     {expected_one_output}")
print(f"Cost of ones:  {errors_of_one}")
#+end_src

#+RESULTS:
: Result of zero: [4.5  4.68]
: Ideal zero:     [1 0]
: Cost of zeros:  [-3.5  -4.68]
: Result of one: [2.16 1.57]
: Ideal one:     [0 1]
: Cost of ones:  [-2.16 -0.57]

This tells us something. It tells us how far off our results are. We don't know what to do with that yet. But at least we have a programatic way to quantify "We need to lower the value of the 'zero' output node twice as much as we need to lower the value of the 'one' output node."

We know we want to adjust our weights so that our prediction gets closer.

We don't know whether to adjust our weights up or down. And we don't know by how much.

Let's just pick a weight and adjust it up and see what happens. If our prediction gets more accurate, we know we're on the right track. If it gets less accurate, then we can simply move the other direction.

#+begin_src python :results output
print(weights)
derivative_of_zero = np.dot(errors_of_zero.reshape(-1, 1), ideal_zero.reshape(1, -1))
print(derivative_of_zero)
derivative_of_one = np.dot(errors_of_one.reshape(-1, 1), ideal_one.reshape(1, -1))
print(derivative_of_one)
#+end_src

#+RESULTS:
: [[0.54 0.62 0.06 0.81 0.86 0.63 0.68 0.68 0.48]
:  [0.73 0.16 0.73 0.22 0.7  0.96 0.28 0.71 0.89]]
: [[-3.5  -3.5  -3.5  -3.5   0.   -3.5  -3.5  -3.5  -3.5 ]
:  [-4.68 -4.68 -4.68 -4.68  0.   -4.68 -4.68 -4.68 -4.68]]
: [[ 0.   -2.16  0.    0.   -2.16  0.    0.   -2.16  0.  ]
:  [ 0.   -0.57  0.    0.   -0.57  0.    0.   -0.57  0.  ]]

We have values that we can use to update our weights. Let's see what happens when we do that.

#+begin_src python :results output
LEARNING_RATE = 0.1
weights_trained_once = weights * derivative_of_zero * LEARNING_RATE
print((hardcoded_weights - weights).sum())
print((hardcoded_weights - weights_trained_once).sum())
#+end_src

#+RESULTS:
: -8.440000000000001
: 6.065240000000001

We are closer to our ideal hardcoded weights. Let's train it again.

#+begin_src python :results output
result_of_zero_after_training_once = np.dot(weights_trained_once, ideal_zero)
errors_of_zero_after_training_once = errors(
    result_of_zero_after_training_once,
    expected_zero_output
)
print(f"Result of zero: {result_of_zero_after_training_once}")
print(f"Ideal zero:     {expected_zero_output}")
print(f"Cost of zeros after no training:    {errors_of_zero}")
print(f"Cost of zeros after training once:  {errors_of_zero_after_training_once}")

derivative_of_zero_after_training_once = np.dot(
    errors_of_zero_after_training_once.reshape(-1, 1),
    ideal_zero.reshape(1, -1)
)
weights_trained_twice = (
    weights
    ,* derivative_of_zero_after_training_once
    ,* LEARNING_RATE
)

result_of_zero_after_training_twice = np.dot(weights_trained_twice, ideal_zero)
errors_of_zero_after_training_twice = errors(
    result_of_zero_after_training_twice,
    expected_zero_output
)
print(f"Result of zero: {result_of_zero_after_training_twice}")
print(f"Ideal zero:     {expected_zero_output}")
print(f"Total error after no training:    {errors_of_zero.sum()}")
print(f"Total error after training once:  {errors_of_zero_after_training_once.sum()}")
print(f"Total error after training twice: {errors_of_zero_after_training_twice.sum()}")
#+end_src

#+begin_src python :results output
print(f"Weights: \n{vector_round(weights_trained_twice, 2)}\n")
#+end_src

#+RESULTS:
: Weights:
: [[0.14 0.16 0.02 0.21 0.   0.16 0.18 0.18 0.12]
:  [0.16 0.04 0.16 0.05 0.   0.21 0.06 0.16 0.19]]


#+begin_src python :results output
def cost(calculated, target):
    return (calculated - target) ** 2

def cost_prime(weights, inputs, target):
    return 2 * np.dot(weights, inputs) - 2 * target

adjustment = np.dot(weights.T, errors_of_one)
new_weights = weights + 0.1 * weights * adjustment
new_weights = np.array(vector_round(new_weights, 2))
print(weights)
print(new_weights)
print(np.array(hardcoded_weights))
#+end_src

#+RESULTS:
: [[0.54 0.62 0.06 0.81 0.86 0.63 0.68 0.68 0.48]
:  [0.73 0.16 0.73 0.22 0.7  0.96 0.28 0.71 0.89]]
: [[0.45 0.53 0.06 0.66 0.67 0.51 0.57 0.55 0.41]
:  [0.61 0.14 0.69 0.18 0.54 0.78 0.23 0.58 0.75]]
: [[0.2 0.  0.2 0.2 0.  0.2 0.2 0.  0.2]
:  [0.  0.1 0.  0.  0.9 0.  0.  0.1 0. ]]

*** Imports
#+begin_src python :noweb-ref imports :results none
import numpy as np
from numpy.random import MT19937

from numpy.random import RandomState, SeedSequence
#+end_src

*** Helper

#+begin_src python :noweb-ref helpers :results none
def flatten(l):
    result = []
    for x in l:
        if not isinstance(x, list):
            result.append(x)
        else:
            result.extend(flatten(x))
    return np.array(result)
#+end_src

** Initialize random weights

~np.random.rand~ creates a matrix of random values between [0, 1). The arguments passed are the sizes of each dimension. ~np.random.rand(2, 3)~ will create a 2x3 matrix of random values.

Each dimension is a numpy array. Numpy arrays behave uniquely with math operators in that the operation is performed on each element of the array.

So ~x~ in the generator below will be a numpy array that looks like ~[0.13328, 0.83111, ...]~ and multiplying ~x~ by ~0.12~ will multiply every element in that numpy array by ~0.12~. The generator is operating on each row and the math operations are operating on each element in the row.

#+NAME: define initialize random weights
#+BEGIN_SRC python
def _initialize_random_weights(self, size_in, size_out):
    """
    Creates a matrix with `size_out` rows and `size_in` columns.
    Values will be randomized between -0.06 and 0.06.
    """
    return np.random.rand(size_in, size_out) * 0.12 - 0.06
#+END_SRC


#+BEGIN_SRC python :noweb yes :tangle nn.py
import csv
from collections import namedtuple
import math
import random
import os
import json
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

class OCRNeuralNetwork:
    LEARNING_RATE = 0.05
    NEURAL_NETWORK_FILE_PATH = "neural_network.json"
    def __init__(
            self,
            num_input_nodes,
            num_hidden_nodes,
            num_output_nodes,
            use_file=True
    ):
        self.num_input_nodes = num_input_nodes
        self.num_hidden_nodes = num_hidden_nodes
        self.num_output_nodes = num_output_nodes
        self.__sigmoid = np.vectorize(self._sigmoid_scalar)
        self.__sigmoid_prime = np.vectorize(self._sigmoid_prime_scalar)
        if not os.path.isfile(self.NEURAL_NETWORK_FILE_PATH):
            self.theta1 = self._initialize_random_weights(num_input_nodes, num_hidden_nodes)
            self.theta2 = self._initialize_random_weights(num_hidden_nodes, num_output_nodes)
            self.input_layer_bias = np.random.rand(num_hidden_nodes) * 0.12 - 0.06
            self.hidden_layer_bias = np.random.rand(num_output_nodes) * 0.12 - 0.06

    <<define initialize random weights>>

    def sigmoid(self, z):
        return self.__sigmoid(np.clip(z, -100, 100))

    def _sigmoid_scalar(self, z):
        """Activation function."""
        return 1 / (1 + math.e ** -z)

    def sigmoid_prime(self, z):
        return self.__sigmoid_prime(np.clip(z, -100, 100))

    def _sigmoid_prime_scalar(self, z):
        return self.sigmoid(z) * (1 - self.sigmoid(z))

    def initialize(self):
        with open("simple_train.csv", "rb") as f:
            data_matrix = np.loadtxt(f, delimiter=",", skiprows=1)
        data_labels = data_matrix[:1000,0]
        data_matrix = data_matrix[:1000,1:]
        # data_matrix = np.where(data_matrix > 160, 1, 0)
        data_with_labels = list(zip(data_matrix, data_labels))
        for data, label in random.choices(data_with_labels, k=1000):
            self.back_propagate(data, int(label))

    def forward_propagate(self, input_vals):
        input_vals = np.array(input_vals)
        y1 = np.dot(input_vals, self.theta1)
        y1 = self.sigmoid(y1)

        y2 = np.dot(y1, self.theta2)
        y2 = self.sigmoid(y2)
        return y2

    def predict(self, test):
        output_node_vals = self.forward_propagate(test)
        return output_node_vals

    def back_propagate(self, input_data, data_label):
        # Step 1. Forward propagate, saving the intermediate values
        # that we'll need for the backprop partial derivative formula later.

        # Save off this pre-activation value. We need it later.
        hidden_layer_pre_activations = (
            np.dot(input_data, self.theta1)
            + self.input_layer_bias
        )
        hidden_layer_activations = self.sigmoid(hidden_layer_pre_activations)

        output_layer_pre_activations = (
            np.dot(hidden_layer_activations, self.theta2)
            + self.hidden_layer_bias
        )
        output_layer_activations = self.sigmoid(output_layer_pre_activations)
        self.output_layer_activations = output_layer_activations

        # Step 2. Back propagate.
        target_values = np.zeros(self.num_output_nodes)
        target_values[data_label] = 1


        # 1 x num_output_nodes
        errors_of_output_layer = output_layer_activations - target_values
        self.errors = errors_of_output_layer

        # num_output_nodes x num_hidden_nodes
        # same dimensions as weights
        rate_of_change_of_error_with_respect_to_final_weights = np.dot(
            (
                errors_of_output_layer
                * self.sigmoid_prime(output_layer_pre_activations)
            ).reshape(-1, 1),
            hidden_layer_activations.reshape(1, -1)
        ).T
        self.rate_of_change_of_error_with_respect_to_final_weights = (
            rate_of_change_of_error_with_respect_to_final_weights
        )

        # 1 x num_hidden_nodes
        errors_of_hidden_layer = np.dot(
            errors_of_output_layer
            * self.sigmoid_prime(output_layer_pre_activations),
            self.theta2.T
        )
        # num_hidden_nodes x num_input_nodes
        # same dimensions as weights
        rate_of_change_of_error_with_respect_to_first_weights = (
            (
                errors_of_hidden_layer  # 1 x num_hidden_nodes
                * self.sigmoid_prime(hidden_layer_pre_activations)  # 1 x num_hidden_nodes
            ).reshape(-1, 1)  # num_hidden_nodes x 1
            * input_data.reshape(1, -1)  # 1 x num_input_nodes
        ).T

        self.theta2 -= (
            self.LEARNING_RATE
            * rate_of_change_of_error_with_respect_to_final_weights
        )
        self.hidden_layer_bias -= errors_of_output_layer * self.LEARNING_RATE
        self.theta1 -= (
            self.LEARNING_RATE
            * rate_of_change_of_error_with_respect_to_first_weights
        )
        self.input_layer_bias -= errors_of_hidden_layer * self.LEARNING_RATE

    def train(self, training_data):
        for data in training_data:
            output_node_vals = self.forward_propagate(data["y0"])

            # Back propagate
            actual_vals = np.zeros(10)
            actual_vals[data["label"]] = 1
            output_errors = actual_vals - output_node_vals
            hidden_errors = np.multiply(
                self.theta2, output_errors,
                self.sigmoid_prime(hidden_layer_node_vals)
            )

            # Update weights
            self.theta1 += self.LEARNING_RATE * (hidden_errors * data["y0"])
            self.theta2 += self.LEARNING_RATE * (output_errors * hidden_layer_node_vals)
            self.hidden_layer_bias += self.LEARNING_RATE * output_errors
            self.input_layer_bias += self.LEARNING_RATE * hidden_errors

    def save(self):
        """
        We need to work with Numpy "array" types, but the `json` library
        that we use to serialize/deserialize doesn't know about Numpy types.
        So, we serialize things as regular python types, like lists, and then
        deserialize them the same way, and then convert them back to Numpy types.
        """
        json_neural_network = {
            "theta1": self.theta1.tolist(),
            "theta2": self.theta2.tolist(),
            "bias1": self.input_layer_bias.tolist(),
            "bias2": self.hidden_layer_bias.tolist(),
        }
        with open(self.NEURAL_NETWORK_FILE_PATH) as f:
            json.dump(json_neural_network, f)

    def load(self):
        """
        We need to work with Numpy "array" types, but the `json` library
        that we use to serialize/deserialize doesn't know about Numpy types.
        So, we serialize things as regular python types, like lists, and then
        deserialize them the same way, and then convert them back to Numpy types.
        """
        if not os.path.isfile(self.NEURAL_NETWORK_FILE_PATH):
            return
        with open(self.NEURAL_NETWORK_FILE_PATH) as f:
            neural_network = json.load(f)
        self.theta1 = np.array(neural_network["theta1"])
        self.theta2 = np.array(neural_network["theta2"])
        self.input_layer_bias = np.array(neural_network["bias1"])
        self.hidden_layer_bias = np.array(neural_network["bias2"])
#+END_SRC
